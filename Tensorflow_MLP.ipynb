{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow-MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnFzrgZaCj4a"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKDCH3wtCsuT"
      },
      "source": [
        "# Creating a MLP from scratch\n",
        "\n",
        "class MyDenseLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(MyDenseLayer, self).__init__()\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    self.W = self.add_weight([input_dim, output_dim])\n",
        "    self.b = self.add_weight([1, output_dim])\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Forward propagate the inputs\n",
        "    z = tf.matmul(inputs, self.W) + self.b\n",
        "\n",
        "    # Feed through a non-linear function\n",
        "    output = tf.math.sigmoid(z)\n",
        "    return output\n",
        "\n",
        "# Alternate way (which is also the easiest) way to creat a layer\n",
        "layer = tf.keras.layers.Dense(units=2)\n",
        "\n",
        "# We can create a multi output perceptron using tf\n",
        "# Here we are creating a sequential model,\n",
        "# The first consists of n layers\n",
        "# The second layers with 2 neurons\n",
        "model = tf.keras.Sequential([tf.keras.layers.Dense(n),\n",
        "                             tf.keras.layers.Dense(2)\n",
        "                             ])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fANYD9twQ7Dg"
      },
      "source": [
        "When we look into binary classfication problems\n",
        "- We can use something called **softmax cross entropy loss**.\n",
        "- Cross entropy loss can be used with models that output a probabilty between 0 and 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OACFzdoNRSRY"
      },
      "source": [
        "loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(y, predicted)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUPjiD9RR0oa"
      },
      "source": [
        "In a regression problem we use M.S.E as a cost function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZSDtkp9RSPK"
      },
      "source": [
        "loss = tf.reduce_mean( tf.square(tf.subtract(y, predicted)) ) # Straight forward implementation\n",
        "loss = tf.keras.losses.MSE( y, predicted ) # Using keras API"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpCwj-ysQ7Ag"
      },
      "source": [
        "Gradient Descent implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKGuCBDsUKHa"
      },
      "source": [
        "weights = tf.Variable([tf.random.normal()])\n",
        "\n",
        "for i in range(5000):\n",
        "  with tf.GradientTape() as g:\n",
        "    loss = compute_loss(weights)\n",
        "    # Computing gradient, also knows as back propagation\n",
        "    gradient = g.gradient(loss, weights)\n",
        "  weights = weights - lr * gradient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQYyk-p9XqG4"
      },
      "source": [
        "There are different kinds of Gradient Descents which tensorflow already has\n",
        "- SGD - `tf.keras.optimizaers.SGD`\n",
        "- Adam - `tf.keras.optimizers.Adam`\n",
        "- Adadelta - `tf.keras.Adadelta`\n",
        "- Adagrad - `tf.keras.Adagrad`\n",
        "- RMSProp - `tf.keras.optimizers.RMSProp`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsAsGlVnYVro"
      },
      "source": [
        "Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT-nT_-KYU6_"
      },
      "source": [
        "model = tf.keras.Sequential([.....])\n",
        "\n",
        "# Pick the appropriate optimizer\n",
        "optimizer = tf.keras.optimizer.SGD()\n",
        "\n",
        "while True: # looping forever\n",
        "  prediction = model(x)\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Computing the loss\n",
        "    loss = compute_loss(y, prediction)\n",
        "\n",
        "  # update the weights using gradient\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradient(zip(grads, model.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}