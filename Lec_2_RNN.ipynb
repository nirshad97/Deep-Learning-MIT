{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lec-2-RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU2uMsLaV5An"
      },
      "source": [
        "## RNN Intuition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7YsgKEzVSAK"
      },
      "source": [
        "my_rnn = RNN() \n",
        "\n",
        "# Initializing a hidden state and a network\n",
        "hidden_state = [0, 0, 0, 0]\n",
        "sentence = [\"I\", \"love\", \"recurrent\", \"neural\"]\n",
        "\n",
        "# Task is to predict the next word in the sentence\n",
        "for word in sentence:\n",
        "  # In each step we are going to feed both the current word and the previous hidden state\n",
        "  prediction, hidden_state = my_rnn(word, hidden_state)\n",
        "  # This is going to generate a prediction for the next word\n",
        "  # Also updates the hidden state itself\n",
        "\n",
        "next_word_prediction = prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCxQ8AVOUKQw"
      },
      "source": [
        "RNNs from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJYswjU4UI4x"
      },
      "source": [
        "import tensorlow as tf\n",
        "\n",
        "class MyRNNCell(tf.keras.layers.Layer):\n",
        "  def __init__(self, rnn_units, input_dim, output_dim):\n",
        "    super(MyRNNCell, self).__init__()\n",
        "\n",
        "    # Initialize weight matrices\n",
        "    self.W_xh = self.add_weight([rnn_units, input_dim])\n",
        "    self.W_hh = self.add_weight([rnn_units, rnn_units])\n",
        "    self.W_hy = self.add_eight([output_dim, rnn_units])\n",
        "\n",
        "    # Initialize hidden state to zeros\n",
        "    self.h = tf.zeros([rnn_units, 1])\n",
        "\n",
        "  # Defining a call function\n",
        "  def call(self, X):\n",
        "    #Update the hidden state\n",
        "    self.h = tf.math.tanh( self.W_hh * self.h + self.W_xh * X )\n",
        "\n",
        "    # Compute the output\n",
        "    outpu = self.W_hy * self.h\n",
        "\n",
        "    #Return the current output and the hidden state for each time step\n",
        "    return output, self.h\n",
        "\n",
        "# Alternate way to create a RNN\n",
        "\n",
        "tf.keras.layers.SimpleRNN(rnn_units)0."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRRLdHrzhQwN"
      },
      "source": [
        "LSTMs\n",
        "- Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems.\n",
        "- A little complex\n",
        "- Tensorflow equivalent is `tf.keras.layers.LSTM(num_units)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNU2F9tQBkLb"
      },
      "source": [
        "### Deep Learning for Sequence Modeling: Summary\n",
        "1. RNNs are well suited for **sequence modeling** tasks\n",
        "2. Model sequences via a **recurrence relation**.\n",
        "3. Training RNNs with **brackpropagation through time**.\n",
        "4. Gated cells like **LSTMs** let us model **long-term dependencies**.\n",
        "5. Models for **music generation**, classification, machine translation and more.\n",
        "\n",
        "\n"
      ]
    }
  ]
}